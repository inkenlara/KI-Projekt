{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from graph import Graph, Part\n",
    "from typing import List, Callable, Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('data/graphs.dat', 'rb') as file:\n",
    "    all_graphs: List[Graph] = pickle.load(file)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(list(map(lambda g: g.get_parts(), all_graphs)), all_graphs, test_size=0.3, random_state=0)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node import Node\n",
    "\n",
    "class Ordering:\n",
    "    def __init__(self, y: List[Graph], degreeAggregate: Callable[[List[int]], int]):\n",
    "        degrees = {}\n",
    "        for graph in y:\n",
    "            for (node, edges) in graph.get_edges().items():\n",
    "                if node.get_part().get_part_id() not in degrees:\n",
    "                    degrees[node.get_part().get_part_id()] = []\n",
    "                degrees[node.get_part().get_part_id()].append(len(edges))\n",
    "        self.keys = {part: degreeAggregate(degs) for (part, degs) in degrees.items()}\n",
    "\n",
    "    def sort(self, x: Set[Part]) -> List[Part]:\n",
    "        sorted_parts = sorted(x, key=lambda n: self.keys[n.get_part_id()])\n",
    "        sorted_parts.reverse()\n",
    "        return sorted_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all mandatory documents\n",
    "ordering = Ordering(Y_train, lambda n: sum(n)/len(n))\n",
    "\n",
    "current_part_id = 0\n",
    "current_graph_id = 0\n",
    "gwp_a = []\n",
    "gwp_graph_indicator = []\n",
    "gwp_graph_labels = []\n",
    "gwp_node_labels = []\n",
    "for x, y in zip(X_train, Y_train):\n",
    "    ordered = ordering.sort(x)\n",
    "    adjacency_matrix = y.get_adjacency_matrix(ordered)\n",
    "\n",
    "    for i in range(adjacency_matrix.shape[0]):\n",
    "        for j in range(adjacency_matrix.shape[1]):\n",
    "            if j < i:\n",
    "                continue\n",
    "            if adjacency_matrix[i][j] == 1:\n",
    "                gwp_a.append((current_part_id+j,current_part_id+i)) # Adjacency of i and j\n",
    "        \n",
    "        gwp_graph_indicator.append(current_graph_id)\n",
    "        gwp_node_labels.append((ordered[i].get_family_id(), ordered[i].get_part_id()))\n",
    "        current_part_id += 1\n",
    "\n",
    "    gwp_graph_labels.append(1)\n",
    "    current_graph_id += 1\n",
    "\n",
    "with open('GRAN/transformed_data/GWP_A.txt', 'a') as f:\n",
    "    for con in gwp_a:\n",
    "        f.write(str(con[0])+\",\"+str(con[1])+\"\\n\")\n",
    "with open('GRAN/transformed_data/GWP_graph_indicator.txt', 'a') as f:\n",
    "    for indicator in gwp_graph_indicator:\n",
    "        f.write(str(indicator)+\"\\n\")\n",
    "with open('GRAN/transformed_data/GWP_graph_labels.txt', 'a') as f:\n",
    "    for label in gwp_graph_labels:\n",
    "        f.write(str(label)+\"\\n\")\n",
    "with open('GRAN/transformed_data/GWP_node_labels.txt', 'a') as f:\n",
    "    for labels in gwp_node_labels:\n",
    "        f.write(str(labels[0])+\",\"+str(labels[1])+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
