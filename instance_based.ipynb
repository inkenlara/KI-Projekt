{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from graph import Graph, Part\n",
    "from typing import List, Set, Tuple, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"data/graphs.dat\", \"rb\") as file:\n",
    "    all_graphs: List[Graph] = pickle.load(file)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        list(map(lambda g: g.get_parts(), all_graphs)),\n",
    "        all_graphs,\n",
    "        test_size=0.3,\n",
    "        random_state=0,\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node import Node\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Create frozen set of part ids\n",
    "def edge(part1: Part, part2: Part) -> Set[int]:\n",
    "    return frozenset({part1.get_part_id(), part2.get_part_id()})\n",
    "\n",
    "\n",
    "# Count how often parts appear together\n",
    "def extract_pairs(graphs: List[Graph]) -> Dict[Set[Part], int]:\n",
    "    pairs = defaultdict(int)\n",
    "    for graph in graphs:\n",
    "        for part1 in graph.get_parts():\n",
    "            for part2 in graph.get_parts():\n",
    "                pairs[edge(part1, part2)] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Count degrees of parts and existing edges\n",
    "def extract_edges(graphs: List[Graph]) -> Tuple[Dict[Part, int], Dict[Set[Part], int]]:\n",
    "    degrees = defaultdict(list)\n",
    "    edges = defaultdict(int)\n",
    "    for graph in graphs:\n",
    "        for node, neighbors in graph.get_edges().items():\n",
    "            degrees[node.get_part().get_part_id()].append(len(neighbors))\n",
    "            for neighbor in neighbors:\n",
    "                edges[edge(node.get_part(), neighbor.get_part())] += 1\n",
    "    return degrees, edges\n",
    "\n",
    "\n",
    "# Edge priority before training is the ratio of edges to occurrences for a pair of parts\n",
    "def init_edge_priority(\n",
    "    pair_count: Dict[Set[Part], int], edge_count: Dict[Set[Part], int]\n",
    "):\n",
    "    edge_priority = defaultdict(float)\n",
    "    for pair, count in pair_count.items():\n",
    "        edge_priority[pair] = edge_count[pair] / count\n",
    "    return edge_priority\n",
    "\n",
    "\n",
    "# Check whether the graph described by edges is a subgraph of the target graph\n",
    "def edges_are_subgraph(edges: List[Tuple[Part, Part]], target: Graph) -> bool:\n",
    "    g = Graph()\n",
    "    for edge0, edge1 in edges:\n",
    "        g.add_undirected_edge(edge0, edge1)\n",
    "    return nx.algorithms.isomorphism.GraphMatcher(\n",
    "        target.to_nx(), g.to_nx(), node_match=lambda a, b: a == b\n",
    "    ).subgraph_is_isomorphic()\n",
    "\n",
    "\n",
    "# Find the first edge that is not part of the target graph\n",
    "def first_wrong_edge(\n",
    "    edges: List[Tuple[Part, Part]], target: Graph\n",
    ") -> Tuple[Part, Part]:\n",
    "    # Use binary search and edges_are_subgraphs to find the first wrong edge:\n",
    "    min = 1\n",
    "    max = len(edges)\n",
    "    while min < max:\n",
    "        mid = (min + max) // 2\n",
    "        if edges_are_subgraph(edges[:mid], target):\n",
    "            min = mid + 1\n",
    "        else:\n",
    "            max = mid\n",
    "    return min - 1\n",
    "\n",
    "\n",
    "class InstanceBased:\n",
    "    def __init__(\n",
    "        self,\n",
    "        y: List[Graph],\n",
    "        order_train_rate=0.1,\n",
    "        order_train_epochs=2,\n",
    "        edge_train_rate=0.001,\n",
    "        edge_train_epochs=10,\n",
    "        edge_pickle_load=False,\n",
    "        edge_pickle_store=False,\n",
    "    ):\n",
    "        pairs_count = extract_pairs(y)\n",
    "        degrees, edge_count = extract_edges(y)\n",
    "        self.edge_priority = init_edge_priority(pairs_count, edge_count)\n",
    "        self.avgDegrees = {part: sum(deg) / len(deg) for (part, deg) in degrees.items()}\n",
    "        self.maxDegrees = {part: max(deg) for (part, deg) in degrees.items()}\n",
    "        self.sort_priority = self.avgDegrees.copy()\n",
    "        self.train_order(y, order_train_rate, order_train_epochs)\n",
    "        if edge_pickle_load:\n",
    "            path = (\n",
    "                edge_pickle_load\n",
    "                if isinstance(edge_pickle_load, str)\n",
    "                else \"data/edge_priority.dat\"\n",
    "            )\n",
    "            with open(path, \"rb\") as file:\n",
    "                self.edge_priority = pickle.load(file)\n",
    "        else:\n",
    "            self.train_edges(y, edge_train_rate, edge_train_epochs)\n",
    "        if edge_pickle_store:\n",
    "            path = (\n",
    "                edge_pickle_store\n",
    "                if isinstance(edge_pickle_store, str)\n",
    "                else \"data/edge_priority.dat\"\n",
    "            )\n",
    "            with open(path, \"wb\") as file:\n",
    "                pickle.dump(self.edge_priority, file)\n",
    "\n",
    "    # Sort parts to allow building tree sequentially\n",
    "    def predict_order(self, x: Set[Part]) -> List[Part]:\n",
    "        return sorted(x, key=lambda n: self.sort_priority[n.get_part_id()], reverse=1)\n",
    "\n",
    "    # Train sorting to enable \"dumbbell\" graphs\n",
    "    def train_order(self, graphs: List[Graph], train_rate, epochs):\n",
    "        print(f\"Possible accuracy when sequentially building a tree using sort method\")\n",
    "        for epoch in range(epochs):\n",
    "            print(\n",
    "                f\"Train order, Epoch {epoch}: {100 * self.evaluate_order(graphs, train_rate) / len(graphs)}% accuracy\"\n",
    "            )\n",
    "        print(\n",
    "            f\"Train order, Epoch {epochs}: {100 * self.evaluate_order(graphs) / len(graphs)}% accuracy\"\n",
    "        )\n",
    "\n",
    "    # (Train and) evaluate supprt of graphs\n",
    "    def evaluate_order(self, graphs: List[Graph], train_rate=False) -> int:\n",
    "        compatible_graphs = 0\n",
    "        for graph in graphs:\n",
    "            compatible = True\n",
    "            edges = {\n",
    "                node: edges.copy() for (node, edges) in graph.get_edges().items()\n",
    "            }  # Copy edges\n",
    "            nodes: List[Node] = sorted(\n",
    "                graph.get_nodes(),\n",
    "                key=lambda n: self.sort_priority[n.get_part().get_part_id()],\n",
    "            )  # Sort nodes reversed\n",
    "\n",
    "            # Check whether leaves can be removed one by one using reversed sorting order\n",
    "            for node in nodes[:-1]:\n",
    "                if len(edges[node]) != 1:\n",
    "                    # If not a leaf, it should've appeared later\n",
    "                    compatible = False\n",
    "                    if train_rate:\n",
    "                        # If training, adapt priority accordingly\n",
    "                        self.sort_priority[node.get_part().get_part_id()] += train_rate\n",
    "                    break\n",
    "                # Remove the leaf\n",
    "                edges[edges[node][0]].remove(node)\n",
    "            compatible_graphs += compatible\n",
    "        return compatible_graphs\n",
    "\n",
    "    def _next_correct_edge(\n",
    "        self, edges: List[Tuple[Part, Part]], part: Part, target: Graph\n",
    "    ) -> Tuple[Part, Part]:\n",
    "        parts = [edges[0][0]] + [edge[1] for edge in edges]\n",
    "        candidates = sorted(\n",
    "            [(p, part) for p in parts],\n",
    "            key=lambda p: self.edge_priority[edge(p[0], p[1])],\n",
    "            reverse=True,\n",
    "        )\n",
    "        for candidate in candidates:\n",
    "            if edges_are_subgraph(edges + [candidate], target):\n",
    "                return candidate\n",
    "\n",
    "    # Sequentially builds graph after sorting parts\n",
    "    def createGraph(\n",
    "        self,\n",
    "        unordered_parts: Set[Part],\n",
    "        avgDegreeInfluence=0.05,\n",
    "        maxDegreeInfluence=0.5,\n",
    "        target=None,\n",
    "        train_rate=None,\n",
    "    ) -> Graph:\n",
    "        parts = self.predict_order(unordered_parts)\n",
    "        edges = [(parts[0], parts[1])]\n",
    "        g = Graph()\n",
    "        g.add_undirected_edge(parts[0], parts[1])\n",
    "        for i in range(2, len(parts)):\n",
    "            best_edge = max(\n",
    "                [(p, parts[i]) for p in parts[:i]],\n",
    "                default=(parts[0], parts[i]),\n",
    "                key=lambda p: self.edge_priority[edge(p[0], p[1])]\n",
    "                + avgDegreeInfluence  # Prefer nodes that require more neighbors\n",
    "                * (self.avgDegrees[p[0].get_part_id()] - g.get_degree(p[0]))\n",
    "                - maxDegreeInfluence  # Dont exceed maxDegree\n",
    "                * (g.get_degree(p[0]) == self.maxDegrees[p[0].get_part_id()]),\n",
    "            )\n",
    "            edges.append(best_edge)\n",
    "            g.add_undirected_edge(best_edge[0], best_edge[1])\n",
    "        if target and train_rate and g != target:\n",
    "            first_wrong = first_wrong_edge(edges, target)\n",
    "            correct = self._next_correct_edge(\n",
    "                edges[:first_wrong], parts[first_wrong + 1], target\n",
    "            )\n",
    "            self.edge_priority[edge(correct[0], correct[1])] += train_rate\n",
    "            # print(edges)\n",
    "            # g.draw()\n",
    "            # print(f\"{correct} instead of {edges[first_wrong]}\")\n",
    "            # target.draw()\n",
    "            return None\n",
    "        return g\n",
    "\n",
    "    def train_edges(self, graphs: List[Graph], train_rate, epochs):\n",
    "        print(f\"Training edge priorities {epochs} epochs: \", end=\"\")\n",
    "        for epoch in range(epochs):\n",
    "            for graph in graphs:\n",
    "                self.createGraph(graph.get_parts(), target=graph, train_rate=train_rate)\n",
    "            print(f\"{epoch + 1} \", end=\"\")\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = InstanceBased(y_train, edge_pickle_store=True)\n",
    "print(f\"Order Validation: {ib.evaluate_order(y_val) / len(y_val)}% accuracy\\n\")\n",
    "\n",
    "y_train_prev_right = []\n",
    "y_train_prev_wrong = []\n",
    "y_val_prev_right = []\n",
    "y_val_prev_wrong = []\n",
    "\n",
    "correct_train = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == ib.createGraph(X_train[i]):\n",
    "        correct_train += 1\n",
    "        y_train_prev_right.append(y_train[i])\n",
    "    else:\n",
    "        y_train_prev_wrong.append(y_train[i])\n",
    "print(f\"train_err: {correct_train / len(y_train)}\")\n",
    "correct_val = 0\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i] == ib.createGraph(X_val[i]):\n",
    "        correct_val += 1\n",
    "        y_val_prev_right.append(y_val[i])\n",
    "    else:\n",
    "        y_val_prev_wrong.append(y_val[i])\n",
    "print(f\"val_err: {correct_val / len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = InstanceBased(y_train, edge_pickle_load=True)\n",
    "\n",
    "correct_train = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == ib.createGraph(X_train[i]):\n",
    "        correct_train += 1\n",
    "print(f\"train_err: {correct_train / len(y_train)}\")\n",
    "correct_val = 0\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i] == ib.createGraph(X_val[i]):\n",
    "        correct_val += 1\n",
    "print(f\"val_err: {correct_val / len(y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current problems:\n",
    "train[5]: 114 is added twice to 36 (fewer but higher probability) instead of once to 36 and once to 182\n",
    "\n",
    "train[14]: Chosen edge is way more probable, would require much more context information\n",
    "\n",
    "train[35]: 218 on 301 instead of duplicate at 1156\n",
    "\n",
    "train_err: 0.9244654973754961\n",
    "val_err: 0.896057347670250\n",
    "\n",
    "\n",
    "Especially slow: train[6283], train[4288], \n",
    "\n",
    "\n",
    "Decomposing the error rate:\n",
    "0.6% incorrect orderings\n",
    "1.9% incorrect saturation (maxDegree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import numpy as np\n",
    "\n",
    "def evaluate( data_set: List[Tuple[Set[Part], Graph]]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates a given prediction model on a given data set.\n",
    "    :param model: prediction model\n",
    "    :param data_set: data set\n",
    "    :return: evaluation score (for now, edge accuracy in percent)\n",
    "    \"\"\"\n",
    "    sum_correct_edges = 0\n",
    "    edges_counter = 0\n",
    "\n",
    "    for input_parts, target_graph in data_set:\n",
    "        predicted_graph = ib.createGraph(input_parts)\n",
    "\n",
    "        edges_counter += len(input_parts) * len(input_parts)\n",
    "        sum_correct_edges += edge_accuracy(predicted_graph, target_graph)\n",
    "\n",
    "    return sum_correct_edges / edges_counter * 100\n",
    "\n",
    "\n",
    "def edge_accuracy(predicted_graph: Graph, target_graph: Graph) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predicted edges.\n",
    "    :param predicted_graph:\n",
    "    :param target_graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(predicted_graph.get_nodes()) == len(target_graph.get_nodes()), 'Mismatch in number of nodes.'\n",
    "    assert predicted_graph.get_parts() == target_graph.get_parts(), 'Mismatch in expected and given parts.'\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    # Determine all permutations for the predicted graph and choose the best one in evaluation\n",
    "    perms: List[Tuple[Part]] = __generate_part_list_permutations(predicted_graph.get_parts())\n",
    "\n",
    "    # Determine one part order for the target graph\n",
    "    target_parts_order = perms[0]\n",
    "    target_adj_matrix = target_graph.get_adjacency_matrix(target_parts_order)\n",
    "\n",
    "    for perm in perms:\n",
    "        predicted_adj_matrix = predicted_graph.get_adjacency_matrix(perm)\n",
    "        score = np.sum(predicted_adj_matrix == target_adj_matrix)\n",
    "        best_score = max(best_score, score)\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def __generate_part_list_permutations(parts: Set[Part]) -> List[Tuple[Part]]:\n",
    "    \"\"\"\n",
    "    Different instances of the same part type may be interchanged in the graph. This method computes all permutations\n",
    "    of parts while taking this into account. This reduced the number of permutations.\n",
    "    :param parts: Set of parts to compute permutations\n",
    "    :return: List of part permutations\n",
    "    \"\"\"\n",
    "    # split parts into sets of same part type\n",
    "    equal_parts_sets: Dict[Part, Set[Part]] = {}\n",
    "    for part in parts:\n",
    "        for seen_part in equal_parts_sets.keys():\n",
    "            if part.equivalent(seen_part):\n",
    "                equal_parts_sets[seen_part].add(part)\n",
    "                break\n",
    "        else:\n",
    "            equal_parts_sets[part] = {part}\n",
    "\n",
    "    multi_occurrence_parts: List[Set[Part]] = [pset for pset in equal_parts_sets.values() if len(pset) > 1]\n",
    "    single_occurrence_parts: List[Part] = [next(iter(pset)) for pset in equal_parts_sets.values() if len(pset) == 1]\n",
    "\n",
    "    full_perms: List[Tuple[Part]] = [()]\n",
    "    for mo_parts in multi_occurrence_parts:\n",
    "        perms = list(permutations(mo_parts))\n",
    "        full_perms = list(perms) if full_perms == [()] else [t1 + t2 for t1 in full_perms for t2 in perms]\n",
    "\n",
    "    # Add single occurrence parts\n",
    "    full_perms = [fp + tuple(single_occurrence_parts) for fp in full_perms]\n",
    "    assert all([len(perm) == len(parts) for perm in full_perms]), 'Mismatching number of elements in permutation(s).'\n",
    "    return full_perms\n",
    "\n",
    "#print(evaluate(list(zip(X_val, y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
